{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T16:24:24.081914400Z",
     "start_time": "2024-11-08T16:24:22.277819500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text category\n0  نظم عهد شرق لفن عرض فنا تحت عنو بقة الف وذل سع...  Culture\n1  تقم فنن ليت كابيلو عرض طلع عام دبي يضم عرض لوح...  Culture\n2  وصل يلة سير تحد تعة ءثر نفس يرق لقب شعر ملي نس...  Culture\n3  عقد ظهر ءمس ءول قصر ثقف شرق جلس ءخر جلس لقى ءو...  Culture\n4  خار صحف يمز جورج ءورويل يحل رتب قءم تضم ءعظم خ...  Culture",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>نظم عهد شرق لفن عرض فنا تحت عنو بقة الف وذل سع...</td>\n      <td>Culture</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>تقم فنن ليت كابيلو عرض طلع عام دبي يضم عرض لوح...</td>\n      <td>Culture</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>وصل يلة سير تحد تعة ءثر نفس يرق لقب شعر ملي نس...</td>\n      <td>Culture</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>عقد ظهر ءمس ءول قصر ثقف شرق جلس ءخر جلس لقى ءو...</td>\n      <td>Culture</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>خار صحف يمز جورج ءورويل يحل رتب قءم تضم ءعظم خ...</td>\n      <td>Culture</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_path = '../resources/dataset.csv'\n",
    "\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Separate texts and labels\n",
    "texts = data['text'].values\n",
    "labels = data['category'].values\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize texts for Word2Vec training\n",
    "train_tokens = [text.split() for text in train_texts]\n",
    "\n",
    "# Train Word2Vec model on training tokens\n",
    "w2v_params = {\"vector_size\": 300, \"window\": 5, \"min_count\": 5, \"workers\": 7}\n",
    "w2v_model = Word2Vec(sentences=train_tokens, **w2v_params)\n",
    "\n",
    "# Create a tokenizer by mapping words to integer IDs using Word2Vec vocabulary\n",
    "tokenizer = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}\n",
    "\n",
    "# Set maximum sequence length based on the longest training sequence\n",
    "max_sequence_length = max(len(tokens) for tokens in train_tokens)\n",
    "\n",
    "# Convert texts to sequences of integers and apply padding\n",
    "train_sequences = [[tokenizer.get(word, 0) for word in text.split()] for text in train_texts]\n",
    "test_sequences = [[tokenizer.get(word, 0) for word in text.split()] for text in test_texts]\n",
    "\n",
    "# Pad sequences to ensure consistent input length and convert to PyTorch tensors\n",
    "train_data = torch.tensor([seq + [0] * (max_sequence_length - len(seq)) for seq in train_sequences], dtype=torch.long)\n",
    "test_data = torch.tensor([seq + [0] * (max_sequence_length - len(seq)) for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = torch.tensor(label_encoder.fit_transform(train_labels), dtype=torch.long)\n",
    "test_labels = torch.tensor(label_encoder.transform(test_labels), dtype=torch.long)\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(label_encoder.classes_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T16:27:25.744578800Z",
     "start_time": "2024-11-08T16:24:24.081914400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karam\\AppData\\Local\\Temp\\ipykernel_18588\\4139540518.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.texts = torch.tensor(texts, dtype=torch.long)\n",
      "C:\\Users\\karam\\AppData\\Local\\Temp\\ipykernel_18588\\4139540518.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = TextDataset(train_data, train_labels)\n",
    "test_dataset = TextDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T16:27:26.489378700Z",
     "start_time": "2024-11-08T16:27:25.760215400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'word_index'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Create embedding matrix\u001B[39;00m\n\u001B[0;32m      2\u001B[0m embedding_dim \u001B[38;5;241m=\u001B[39m w2v_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvector_size\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m embedding_matrix \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;28mlen\u001B[39m(\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_index\u001B[49m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, embedding_dim))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word, i \u001B[38;5;129;01min\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mword_index\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m w2v_model\u001B[38;5;241m.\u001B[39mwv:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'dict' object has no attribute 'word_index'"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix\n",
    "embedding_dim = w2v_params[\"vector_size\"]\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Convert embedding matrix to PyTorch tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T16:28:43.163067500Z",
     "start_time": "2024-11-08T16:28:43.092354500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_save_path = 'Backend/Models/CNN'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the file path for saving the Word2Vec model\n",
    "w2v_model_file_path = os.path.join(model_save_path, 'word2vec.model')\n",
    "\n",
    "# Save the Word2Vec model\n",
    "w2v_model.save(w2v_model_file_path)\n",
    "\n",
    "print(f\"Word2Vec model saved to {w2v_model_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-08T16:28:13.346159500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len, num_classes, embedding_matrix=None):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Convolutional layer and pooling\n",
    "        self.conv = nn.Conv1d(embedding_dim, 128, kernel_size=3)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)  # Reshape for Conv1d\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x).squeeze(2)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TextCNN(vocab_size=len(tokenizer.word_index) + 1,\n",
    "                embedding_dim=embedding_dim,\n",
    "                max_len=max_sequence_length,\n",
    "                num_classes=7,\n",
    "                embedding_matrix=embedding_matrix)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-08T16:28:13.346159500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        texts = texts.to(device, dtype=torch.long)\n",
    "        labels = labels.to(device, dtype=torch.long)  \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(texts)  \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-08T16:28:13.374409900Z",
     "start_time": "2024-11-08T16:28:13.358779200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "test_precision = precision_score(all_labels, all_predictions, average=\"macro\")\n",
    "test_recall = recall_score(all_labels, all_predictions, average=\"macro\")\n",
    "test_f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-08T16:28:13.358779200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define path\n",
    "model_save_path = '/Backend/Models/CNN'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Define file path for model\n",
    "model_file_path = os.path.join(model_save_path, 'cnn_model.pth')\n",
    "\n",
    "# Save model\n",
    "torch.save(model, model_file_path)\n",
    "\n",
    "print(f\"Model saved to {model_file_path}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-08T16:28:13.358779200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArabicNewsClassifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
